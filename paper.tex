\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{geometry}
\geometry{margin=1in}

\title{Graph Transformers at Scale: A Comprehensive Benchmark of Global, Sparse, and Hybrid Attention Mechanisms}
\author{Your Name \and Your Partner's Name}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Graph Transformers have emerged as powerful alternatives to traditional Graph Neural Networks (GNNs), offering improved expressiveness through global attention mechanisms. However, the quadratic complexity of full attention limits scalability. This work presents a comprehensive empirical evaluation of scalable graph transformer architectures, comparing GOAT (approximate global attention with virtual nodes) and Exphormer (sparse attention via expander graphs) against strong GNN baselines and hybrid models. We evaluate on the ZINC molecular property prediction benchmark, analyzing complexity-accuracy tradeoffs, computational efficiency, and model performance. Our results demonstrate that hybrid models combining GNN architectures with virtual nodes achieve the best performance (GIN+VN: 0.342 MAE), while graph transformers show competitive results (GOAT: 0.526 MAE, Exphormer: 0.622 MAE) with different computational characteristics. We provide detailed complexity analysis including memory usage, training time, and parameter counts, revealing important tradeoffs between model expressiveness and computational cost.
\end{abstract}

\section{Introduction}

Graph Neural Networks (GNNs) have achieved remarkable success in learning representations from graph-structured data. Traditional GNNs like Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT) operate through local message passing, aggregating information from neighboring nodes. While effective, these local operations can struggle with long-range dependencies and may not capture global graph structure effectively.

Graph Transformers address these limitations by employing global attention mechanisms that allow each node to attend to all other nodes in the graph. However, the quadratic complexity $O(N^2)$ of full self-attention makes these models computationally prohibitive for large graphs. Recent work has proposed several scalable alternatives:

\begin{itemize}
    \item \textbf{GOAT} \cite{kong2023goat}: Approximates global attention using virtual nodes, achieving $O(N)$ complexity
    \item \textbf{Exphormer} \cite{shirzad2023exphormer}: Uses expander graph-based sparse attention, achieving $O(Nd)$ complexity
\end{itemize}

This work presents a comprehensive benchmark comparing these scalable graph transformers against traditional GNN baselines and hybrid models that combine GNN architectures with virtual nodes. Our contributions include:

\begin{enumerate}
    \item A systematic evaluation of 8 models (2 graph transformers, 4 baselines, 2 hybrid) on the ZINC molecular property prediction dataset
    \item Detailed complexity analysis including memory usage, training time, and parameter counts
    \item Analysis of complexity-accuracy tradeoffs across different model architectures
    \item Empirical insights into when graph transformers provide advantages over traditional GNNs
\end{enumerate}

\section{Related Work}

\subsection{Graph Neural Networks}
Traditional GNNs operate through local message passing. GCN \cite{kipf2017gcn} uses spectral graph convolutions, while GAT \cite{velickovic2018gat} introduces attention mechanisms to weight neighbor contributions. GIN \cite{xu2019gin} provides theoretical guarantees for graph isomorphism testing through injective aggregation functions.

\subsection{Scalable Graph Transformers}
The quadratic complexity of full self-attention has motivated several scalable alternatives. GOAT \cite{kong2023goat} introduces virtual nodes that aggregate global information, enabling efficient global attention. Exphormer \cite{shirzad2023exphormer} leverages expander graphs to create sparse attention patterns that maintain connectivity properties while reducing computational cost.

\subsection{Hybrid Architectures}
Virtual nodes have been successfully combined with GNN architectures to create hybrid models that capture both local and global information \cite{gilmer2017neural}. These models add learnable virtual nodes that connect to all graph nodes, enabling global information flow.

\section{Methods}

\subsection{Graph Transformers}

\subsubsection{GOAT (Global Transformer)}
GOAT approximates global attention using virtual nodes. The model maintains $k$ virtual nodes that aggregate information from all graph nodes. Each node attends to its neighbors and the virtual nodes, achieving $O(N)$ complexity instead of $O(N^2)$. The attention mechanism is:

\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

where $Q$, $K$, $V$ are query, key, and value matrices, and the attention is computed over local neighbors plus virtual nodes.

\subsubsection{Exphormer}
Exphormer uses expander graph-based sparse attention. It constructs a sparse attention pattern using expander graphs, which maintain strong connectivity properties while reducing the number of attended nodes to $O(d)$ per node, where $d$ is the expander degree. This achieves $O(Nd)$ complexity.

\subsection{Baseline Models}

\subsubsection{GCN (Graph Convolutional Network)}
GCN uses spectral graph convolutions with a simplified first-order approximation:

\begin{equation}
H^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})
\end{equation}

where $\tilde{A} = A + I$ is the adjacency matrix with self-loops, $\tilde{D}$ is the degree matrix, and $W^{(l)}$ are learnable weights.

\subsubsection{GAT (Graph Attention Network)}
GAT uses attention to weight neighbor contributions:

\begin{equation}
h_i^{(l+1)} = \sigma\left(\sum_{j \in \mathcal{N}(i)} \alpha_{ij} W h_j^{(l)}\right)
\end{equation}

where $\alpha_{ij}$ are attention coefficients computed via a learnable attention mechanism.

\subsubsection{GIN (Graph Isomorphism Network)}
GIN uses injective aggregation functions to ensure maximum discriminative power:

\begin{equation}
h_v^{(k)} = \text{MLP}^{(k)}\left((1 + \epsilon^{(k)}) \cdot h_v^{(k-1)} + \sum_{u \in \mathcal{N}(v)} h_u^{(k-1)}\right)
\end{equation}

\subsubsection{GraphMLP}
GraphMLP is a baseline that ignores graph structure, using only node features with MLP layers.

\subsection{Hybrid Models}

\subsubsection{GCN+VN and GIN+VN}
These hybrid models combine GCN/GIN architectures with virtual nodes. A learnable virtual node is added that connects to all graph nodes, enabling global information flow while maintaining the local message passing of the base architecture.

\section{Experimental Setup}

\subsection{Dataset}
We evaluate on the ZINC dataset \cite{dwivedi2020benchmarking}, a molecular property prediction benchmark containing 12,000 molecular graphs. The task is regression, predicting the constrained solubility property of molecules. We use Mean Absolute Error (MAE) as the evaluation metric.

\subsection{Model Configurations}
All models use the following hyperparameters:
\begin{itemize}
    \item Hidden dimension: 256
    \item Number of layers: 5
    \item Number of attention heads: 8 (for transformer and attention-based models)
    \item Dropout: 0.1
    \item Positional encoding dimension: 16 (Laplacian eigenvectors)
    \item Batch size: 64
    \item Maximum epochs: 200 (with early stopping, patience=20)
\end{itemize}

\subsection{Training Details}
Models are trained with:
\begin{itemize}
    \item Optimizer: Adam with model-specific learning rates
    \item Learning rates: 1e-4 (base), 5e-4 (GOAT), 3e-4 (Exphormer), 1e-3 (GIN, GIN+VN, GCN+VN)
    \item Weight decay: 1e-5
    \item Learning rate schedule: Cosine annealing with 5-epoch warmup for transformers
    \item Early stopping: Enabled with patience of 20 epochs
    \item Gradient clipping: 1.0
\end{itemize}

\subsection{Evaluation}
We report validation MAE, model parameters, training time, and peak memory usage. All experiments are run on NVIDIA RTX 3090 GPU with 24GB memory.

\section{Results}

\subsection{Overall Performance}

Table \ref{tab:results} presents the complete results for all models on the ZINC dataset.

\begin{table}[H]
\centering
\caption{Model Performance on ZINC Dataset}
\label{tab:results}
\begin{tabular}{lcccc}
\toprule
Model & Val MAE & Parameters & Time (s) & Memory (MB) \\
\midrule
GIN+VN & \textbf{0.342} & 1,522,442 & 351.3 & 127.4 \\
GIN & 0.374 & 925,958 & 179.6 & 99.3 \\
GCN+VN & 0.467 & 928,773 & 329.1 & 85.9 \\
GOAT & 0.526 & 5,008,897 & 1446.1 & 316.6 \\
Exphormer & 0.622 & 5,337,862 & 373.7 & 588.5 \\
GraphMLP & 0.755 & 263,937 & 99.5 & 59.3 \\
GCN & 0.768 & 332,289 & 147.7 & 59.0 \\
GAT & 0.781 & 334,849 & 34.4 & 104.4 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Key Findings}

\subsubsection{Best Performing Models}
The hybrid model GIN+VN achieves the best performance with 0.342 MAE, demonstrating that combining the expressive power of GIN with global information flow through virtual nodes is highly effective for molecular property prediction. GIN alone achieves 0.374 MAE, showing strong baseline performance.

\subsubsection{Graph Transformers}
GOAT achieves 0.526 MAE, significantly better than Exphormer (0.622 MAE). This suggests that the virtual node-based global attention in GOAT is more effective than the expander graph-based sparse attention for this task. However, both transformers require more parameters and computational resources than GNN baselines.

\subsubsection{Complexity-Accuracy Tradeoffs}
Figure \ref{fig:complexity} illustrates the complexity-accuracy tradeoffs. GIN+VN achieves the best accuracy with moderate complexity (1.5M parameters). Graph transformers (GOAT, Exphormer) have high parameter counts (5M+) but do not achieve the best accuracy, suggesting diminishing returns for this dataset size.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{model_comparison.png}
\caption{Model comparison showing validation MAE (left), training time (middle), and parameter count vs. accuracy (right) for all models on ZINC dataset.}
\label{fig:complexity}
\end{figure}

\subsection{Computational Complexity Analysis}

\subsubsection{Training Time}
GOAT requires the longest training time (1446s), followed by Exphormer (374s). The hybrid models (GIN+VN, GCN+VN) require moderate training time (329-351s), while GNN baselines are fastest (34-180s).

\subsubsection{Memory Usage}
Exphormer has the highest peak memory usage (588.5 MB), likely due to the sparse attention pattern implementation. GOAT uses 316.6 MB, while GNN baselines use 59-127 MB.

\subsubsection{Parameter Efficiency}
GIN+VN achieves the best accuracy with 1.5M parameters, making it highly parameter-efficient. Graph transformers use 5M+ parameters but achieve lower accuracy, indicating overparameterization for this task.

\subsection{Learning Curves}
Figure \ref{fig:learning_curves} shows training and validation curves for all models. GIN and GIN+VN show stable convergence with smooth learning curves. GOAT and Exphormer show more variability, particularly in early epochs, which may indicate the need for more careful hyperparameter tuning.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{learning_curves.png}
\caption{Learning curves showing training loss (blue, left axis) and validation MAE (red, right axis) over epochs for all models. Each subplot shows one model with its best validation MAE indicated in the title.}
\label{fig:learning_curves}
\end{figure}

\section{Analysis}

\subsection{Complexity vs. Accuracy Tradeoffs}

Our results reveal important complexity-accuracy tradeoffs:

\begin{enumerate}
    \item \textbf{Hybrid models provide the best tradeoff}: GIN+VN achieves the best accuracy (0.342 MAE) with moderate complexity (1.5M parameters, 351s training time).
    
    \item \textbf{Graph transformers are overparameterized}: GOAT and Exphormer use 5M+ parameters but achieve lower accuracy than GIN+VN. This suggests that for the ZINC dataset size (~12K graphs), the additional expressiveness of global attention does not justify the computational cost.
    
    \item \textbf{Simple baselines are competitive}: GIN achieves 0.374 MAE with only 926K parameters, demonstrating that well-designed local message passing can be highly effective.
\end{enumerate}

\subsection{When Do Graph Transformers Help?}

Based on our results, graph transformers may be more beneficial for:
\begin{itemize}
    \item Larger graphs where long-range dependencies are critical
    \item Tasks requiring explicit modeling of global graph structure
    \item Datasets with more complex graph patterns
\end{itemize}

For molecular property prediction on ZINC, local message passing with virtual nodes (hybrid models) appears sufficient.

\subsection{Limitations and Future Work}

Our study has several limitations:
\begin{itemize}
    \item Evaluation limited to ZINC dataset; results may differ on other benchmarks
    \item No analysis of homophily/heterophily robustness (future work)
    \item Limited ablation studies on positional encodings and virtual node designs
    \item Single random seed; results should be averaged over multiple seeds
\end{itemize}

Future work should:
\begin{itemize}
    \item Evaluate on larger and more diverse graph datasets (OGB, Peptides)
    \item Conduct systematic ablation studies on architectural components
    \item Analyze performance on homophilic vs. heterophilic graphs
    \item Investigate the role of positional encodings in transformer performance
\end{itemize}

\section{Conclusion}

We presented a comprehensive benchmark of scalable graph transformers (GOAT, Exphormer) against GNN baselines and hybrid models on the ZINC molecular property prediction dataset. Our key findings:

\begin{enumerate}
    \item Hybrid models (GIN+VN) achieve the best performance (0.342 MAE) with moderate computational cost
    \item Graph transformers show competitive results but require significantly more parameters and training time
    \item Well-designed local message passing (GIN) remains highly effective for molecular property prediction
    \item Complexity-accuracy tradeoffs favor hybrid architectures for this dataset size
\end{enumerate}

These results suggest that for molecular property prediction, combining expressive GNN architectures with virtual nodes provides the best balance of accuracy and efficiency. Graph transformers may be more beneficial for larger graphs or tasks requiring explicit global structure modeling.

\section*{Acknowledgments}
We thank the authors of GOAT and Exphormer for making their code publicly available.

\bibliographystyle{unsrt}
\begin{thebibliography}{99}

\bibitem{kong2023goat}
D. Kong, H. Liu, P. Konda, et al. GOAT: A Global Transformer on Large-scale Graphs. ICML 2023.

\bibitem{shirzad2023exphormer}
H. Shirzad, A. Velingker, B. Venkatachalam, D. Sutherland, A. K. Sinop. Exphormer: Sparse Transformers for Graphs. ICML 2023.

\bibitem{kipf2017gcn}
T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. ICLR 2017.

\bibitem{velickovic2018gat}
P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio. Graph attention networks. ICLR 2018.

\bibitem{xu2019gin}
K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? ICLR 2019.

\bibitem{gilmer2017neural}
J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. ICML 2017.

\bibitem{dwivedi2020benchmarking}
V. P. Dwivedi, C. K. Joshi, T. Laurent, Y. Bengio, and X. Bresson. Benchmarking graph neural networks. NeurIPS 2020.

\end{thebibliography}

\end{document}
