\documentclass[11pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{setspace}

% Page setup
\geometry{margin=1in}
\onehalfspacing

% Custom colors
\definecolor{primaryblue}{RGB}{41, 98, 255}
\definecolor{secondarygray}{RGB}{100, 100, 100}

% Title formatting
\titleformat{\chapter}[display]
{\normalfont\huge\bfseries\color{primaryblue}}
{\chaptertitlename\ \thechapter}{20pt}{\Huge}

\titleformat{\section}
{\normalfont\Large\bfseries\color{primaryblue}}
{\thesection}{1em}{}

\titleformat{\subsection}
{\normalfont\large\bfseries}
{\thesubsection}{1em}{}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small Graph Transformers at Scale}
\fancyhead[R]{\small \thepage}
\fancyfoot[C]{}
\renewcommand{\headrulewidth}{0.4pt}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=primaryblue,
    filecolor=primaryblue,
    urlcolor=primaryblue,
    citecolor=primaryblue
}

\title{\textbf{Graph Transformers at Scale:}\\
\large A Comprehensive Benchmark of Global, Sparse, and Hybrid Attention Mechanisms}
\author{}
\date{\today}

\begin{document}

\maketitle
\thispagestyle{empty}

\newpage
\tableofcontents
\newpage

\begin{abstract}
\noindent
Graph Transformers have emerged as powerful alternatives to traditional Graph Neural Networks (GNNs), offering improved expressiveness through global attention mechanisms. However, the quadratic complexity of full attention limits scalability. 

This work presents a comprehensive empirical evaluation of scalable graph transformer architectures, comparing GOAT (approximate global attention with virtual nodes) and Exphormer (sparse attention via expander graphs) against strong GNN baselines and hybrid models. We evaluate on the ZINC molecular property prediction benchmark, analyzing complexity-accuracy tradeoffs, computational efficiency, and model performance. 

Our results demonstrate that hybrid models combining GNN architectures with virtual nodes achieve the best performance (GIN+VN: 0.342 MAE), while graph transformers show competitive results (GOAT: 0.526 MAE, Exphormer: 0.622 MAE) with different computational characteristics. We provide detailed complexity analysis including memory usage, training time, and parameter counts, revealing important tradeoffs between model expressiveness and computational cost.
\end{abstract}

\chapter{Introduction}

Graph Neural Networks (GNNs) have revolutionized how we learn from graph-structured data. Traditional approaches like Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT) work by passing messages between neighboring nodes—think of it like information spreading through a social network, where each person only talks to their immediate friends. While this works well for many tasks, it can struggle when information needs to travel across the entire graph.

Enter Graph Transformers. These models use a mechanism called \textit{global attention}, which allows every node to "see" and interact with every other node in the graph simultaneously. Imagine if instead of only talking to immediate friends, everyone in the network could communicate with everyone else at once. This global view can capture complex patterns that local message passing might miss.

However, there's a catch: computing attention between all pairs of nodes becomes extremely expensive as graphs grow larger. The computational cost scales quadratically with the number of nodes—for a graph with 1000 nodes, we'd need to compute 1,000,000 attention scores! This makes full global attention impractical for large graphs.

Recent research has proposed clever solutions to make graph transformers scalable:

\begin{itemize}
    \item \textbf{GOAT} \cite{kong2023goat}: Uses "virtual nodes" that act as information hubs, allowing efficient global communication while maintaining $O(N)$ complexity
    \item \textbf{Exphormer} \cite{shirzad2023exphormer}: Uses mathematical structures called expander graphs to create sparse but well-connected attention patterns, achieving $O(Nd)$ complexity
\end{itemize}

In this report, we conduct a comprehensive benchmark comparing these scalable graph transformers against traditional GNN baselines and hybrid models that combine the best of both worlds. Our main contributions are:

\begin{enumerate}
    \item A systematic evaluation of 8 different models (2 graph transformers, 4 baselines, 2 hybrid) on the ZINC molecular property prediction dataset
    \item Detailed complexity analysis examining memory usage, training time, and parameter counts
    \item Analysis of the tradeoffs between model complexity and accuracy
    \item Practical insights into when graph transformers provide real advantages over traditional GNNs
\end{enumerate}

\chapter{Background and Related Work}

\section{Graph Neural Networks}

Traditional GNNs operate through a process called \textit{message passing}. In each layer, nodes aggregate information from their neighbors, update their own representations, and pass messages to the next layer. This creates a local view of the graph structure.

\begin{itemize}
    \item \textbf{GCN} \cite{kipf2017gcn}: Uses spectral graph convolutions, treating graph operations similar to how convolutional neural networks work on images
    \item \textbf{GAT} \cite{velickovic2018gat}: Introduces attention mechanisms, allowing nodes to weight the importance of different neighbors
    \item \textbf{GIN} \cite{xu2019gin}: Designed with theoretical guarantees for distinguishing different graph structures, making it particularly powerful for graph classification tasks
\end{itemize}

While these models excel at capturing local patterns, they can struggle with long-range dependencies—information that needs to propagate across many hops in the graph.

\section{Scalable Graph Transformers}

The challenge of making transformers work on graphs lies in their computational cost. Several approaches have emerged:

\begin{itemize}
    \item \textbf{GOAT} \cite{kong2023goat}: Introduces virtual nodes that aggregate global information. Think of them as special nodes that connect to everyone, creating shortcuts for global communication
    \item \textbf{Exphormer} \cite{shirzad2023exphormer}: Uses expander graphs—mathematical structures that maintain strong connectivity with few edges—to create efficient sparse attention patterns
\end{itemize}

\section{Hybrid Architectures}

Hybrid models combine the local message passing of GNNs with the global connectivity of virtual nodes \cite{gilmer2017neural}. These models add learnable virtual nodes that connect to all graph nodes, enabling global information flow while maintaining the efficiency of local operations.

\chapter{Methods}

\section{Graph Transformers}

\subsection{GOAT: Global Attention with Virtual Nodes}

GOAT achieves global attention efficiently by introducing virtual nodes. Instead of every node attending to every other node (which would be $O(N^2)$), each node attends to:
\begin{itemize}
    \item Its local neighbors (as in traditional GNNs)
    \item A small number of virtual nodes (typically 1-2)
\end{itemize}

The virtual nodes aggregate information from all nodes, creating a global information hub. The attention mechanism works as:

\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

where $Q$, $K$, $V$ are query, key, and value matrices. By computing attention over neighbors plus virtual nodes instead of all nodes, GOAT achieves $O(N)$ complexity.

\subsection{Exphormer: Sparse Attention via Expander Graphs}

Exphormer takes a different approach. Instead of using virtual nodes, it constructs a sparse attention pattern using expander graphs. These are mathematical structures that maintain strong connectivity properties while using relatively few edges.

The key insight: you don't need to attend to all nodes—you just need to attend to a well-chosen subset that ensures information can flow efficiently throughout the graph. Exphormer reduces the number of attended nodes to $O(d)$ per node, where $d$ is the expander degree, achieving $O(Nd)$ complexity.

\section{Baseline Models}

\subsection{GCN: Graph Convolutional Network}

GCN uses spectral graph convolutions, which can be thought of as a smoothed version of local averaging. The update rule is:

\begin{equation}
H^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})
\end{equation}

where $\tilde{A} = A + I$ adds self-loops to the adjacency matrix, $\tilde{D}$ is the degree matrix, and $W^{(l)}$ are learnable weights. This creates a normalized aggregation of neighbor features.

\subsection{GAT: Graph Attention Network}

GAT introduces learnable attention weights, allowing nodes to focus more on important neighbors:

\begin{equation}
h_i^{(l+1)} = \sigma\left(\sum_{j \in \mathcal{N}(i)} \alpha_{ij} W h_j^{(l)}\right)
\end{equation}

where $\alpha_{ij}$ are attention coefficients that determine how much node $i$ should pay attention to neighbor $j$. These coefficients are learned during training.

\subsection{GIN: Graph Isomorphism Network}

GIN is designed to be maximally expressive for distinguishing different graph structures. It uses injective aggregation functions:

\begin{equation}
h_v^{(k)} = \text{MLP}^{(k)}\left((1 + \epsilon^{(k)}) \cdot h_v^{(k-1)} + \sum_{u \in \mathcal{N}(v)} h_u^{(k-1)}\right)
\end{equation}

The key innovation is the learnable parameter $\epsilon^{(k)}$, which allows the model to balance self-information with neighbor information.

\subsection{GraphMLP: Structure-Agnostic Baseline}

GraphMLP serves as a simple baseline that completely ignores graph structure. It treats nodes independently, using only node features with multi-layer perceptrons. This helps us understand how much the graph structure actually matters.

\section{Hybrid Models}

\subsection{GCN+VN and GIN+VN}

These hybrid models combine the best of both worlds: the local message passing of GCN/GIN with the global connectivity of virtual nodes. A learnable virtual node is added that connects to all graph nodes, creating a global information highway while maintaining the efficiency of local operations.

The virtual node acts as a shared memory that all nodes can read from and write to, enabling efficient global information exchange without the quadratic cost of full attention.

\chapter{Experimental Setup}

\section{Dataset}

We evaluate all models on the ZINC dataset \cite{dwivedi2020benchmarking}, a well-established benchmark for molecular property prediction. The dataset contains 12,000 molecular graphs, where each molecule is represented as a graph with atoms as nodes and bonds as edges.

The task is regression: predicting the constrained solubility property of molecules. This is a real-world application where understanding molecular structure is crucial for drug discovery and material science. We use Mean Absolute Error (MAE) as our evaluation metric—lower is better.

\section{Model Configurations}

To ensure fair comparison, all models use consistent hyperparameters:

\begin{itemize}
    \item \textbf{Hidden dimension}: 256 (the size of node representations)
    \item \textbf{Number of layers}: 5 (depth of the network)
    \item \textbf{Attention heads}: 8 (for transformer and attention-based models)
    \item \textbf{Dropout}: 0.1 (regularization to prevent overfitting)
    \item \textbf{Positional encoding dimension}: 16 (Laplacian eigenvectors to encode graph structure)
    \item \textbf{Batch size}: 64 (number of graphs processed together)
    \item \textbf{Maximum epochs}: 200 (with early stopping to prevent overfitting)
\end{itemize}

\section{Training Details}

We use careful training procedures to ensure fair comparison:

\begin{itemize}
    \item \textbf{Optimizer}: Adam with model-specific learning rates
    \item \textbf{Learning rates}: 
    \begin{itemize}
        \item Base: 1e-4
        \item GOAT: 5e-4 (higher for transformers)
        \item Exphormer: 3e-4 (moderate for stability)
        \item GIN, GIN+VN, GCN+VN: 1e-3 (higher for GNN-based models)
    \end{itemize}
    \item \textbf{Weight decay}: 1e-5 (L2 regularization)
    \item \textbf{Learning rate schedule}: Cosine annealing with 5-epoch warmup for transformers
    \item \textbf{Early stopping}: Enabled with patience of 20 epochs (stops if no improvement)
    \item \textbf{Gradient clipping}: 1.0 (prevents exploding gradients)
\end{itemize}

\section{Evaluation Metrics}

We report several metrics to understand both performance and efficiency:

\begin{itemize}
    \item \textbf{Validation MAE}: Primary performance metric (lower is better)
    \item \textbf{Parameter count}: Model size (fewer is generally better)
    \item \textbf{Training time}: Computational cost (shorter is better)
    \item \textbf{Peak memory usage}: Memory efficiency (lower is better)
\end{itemize}

All experiments were run on an NVIDIA RTX 3090 GPU with 24GB memory, ensuring consistent hardware conditions.

\chapter{Results}

\section{Overall Performance}

Table \ref{tab:results} shows the complete results for all models on the ZINC dataset, sorted by validation MAE (best to worst).

\begin{table}[H]
\centering
\caption{Model Performance on ZINC Dataset}
\label{tab:results}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Val MAE} & \textbf{Parameters} & \textbf{Time (s)} & \textbf{Memory (MB)} \\
\midrule
GIN+VN & \textbf{0.342} & 1,522,442 & 351.3 & 127.4 \\
GIN & 0.374 & 925,958 & 179.6 & 99.3 \\
GCN+VN & 0.467 & 928,773 & 329.1 & 85.9 \\
GOAT & 0.526 & 5,008,897 & 1446.1 & 316.6 \\
Exphormer & 0.622 & 5,337,862 & 373.7 & 588.5 \\
GraphMLP & 0.755 & 263,937 & 99.5 & 59.3 \\
GCN & 0.768 & 332,289 & 147.7 & 59.0 \\
GAT & 0.781 & 334,849 & 34.4 & 104.4 \\
\bottomrule
\end{tabular}
\end{table}

\section{Key Findings}

\subsection{The Winner: Hybrid Models}

The hybrid model \textbf{GIN+VN} achieves the best performance with 0.342 MAE, demonstrating that combining the expressive power of GIN with global information flow through virtual nodes is highly effective for molecular property prediction. 

Interestingly, GIN alone achieves 0.374 MAE—only slightly worse—showing that GIN is already a very strong baseline. The virtual node adds a small but meaningful improvement by enabling global information exchange.

\subsection{Graph Transformers: Competitive but Costly}

GOAT achieves 0.526 MAE, significantly better than Exphormer (0.622 MAE). This suggests that the virtual node-based global attention in GOAT is more effective than the expander graph-based sparse attention for this particular task.

However, both transformers require substantially more parameters (5M+) and computational resources than the GNN baselines. GOAT takes over 24 minutes to train, compared to just 3-6 minutes for the hybrid models.

\subsection{The Complexity-Accuracy Tradeoff}

Figure \ref{fig:complexity} illustrates an important pattern: \textbf{more parameters don't always mean better performance}. GIN+VN achieves the best accuracy with moderate complexity (1.5M parameters), while graph transformers use 5M+ parameters but achieve lower accuracy.

This suggests that for the ZINC dataset size (~12K graphs), the additional expressiveness of global attention doesn't justify the computational cost. The hybrid approach provides a better balance.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{../model_comparison.png}
\caption{Model comparison showing validation MAE (left), training time (middle), and parameter count vs. accuracy (right) for all models on ZINC dataset.}
\label{fig:complexity}
\end{figure}

\section{Computational Analysis}

\subsection{Training Time}

Training time varies dramatically across models:
\begin{itemize}
    \item \textbf{Fastest}: GAT (34 seconds) and GraphMLP (100 seconds)
    \item \textbf{Moderate}: Hybrid models (329-351 seconds, ~5-6 minutes)
    \item \textbf{Slowest}: GOAT (1446 seconds, ~24 minutes) and Exphormer (374 seconds, ~6 minutes)
\end{itemize}

The graph transformers, especially GOAT, require significantly more computation. This is the price of global attention, even when made efficient through virtual nodes or sparse patterns.

\subsection{Memory Usage}

Memory consumption also varies:
\begin{itemize}
    \item \textbf{Most efficient}: GCN and GraphMLP (~59 MB)
    \item \textbf{Moderate}: GIN and hybrid models (85-127 MB)
    \item \textbf{Most memory-intensive}: Exphormer (588 MB) and GOAT (317 MB)
\end{itemize}

Exphormer's high memory usage is somewhat surprising given its sparse attention design. This may be due to implementation details of the expander graph construction.

\subsection{Parameter Efficiency}

\textbf{GIN+VN is the clear winner in parameter efficiency}: it achieves the best accuracy (0.342 MAE) with only 1.5M parameters. In contrast, graph transformers use 5M+ parameters but achieve lower accuracy, indicating overparameterization for this task.

This is an important practical consideration: if you're deploying models in resource-constrained environments, the hybrid models offer much better efficiency.

\section{Learning Curves}

Figure \ref{fig:learning_curves} shows how models learn over time. Several interesting patterns emerge:

\begin{itemize}
    \item \textbf{GIN and GIN+VN}: Show stable, smooth convergence with consistent improvement
    \item \textbf{GOAT and Exphormer}: Show more variability, especially in early epochs, suggesting they may need more careful hyperparameter tuning
    \item \textbf{Baselines}: GraphMLP and GCN show steady but limited improvement, plateauing earlier
\end{itemize}

The smooth learning curves of GIN-based models suggest they're easier to train and more stable, which is valuable in practice.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{../learning_curves.png}
\caption{Learning curves showing training loss (blue, left axis) and validation MAE (red, right axis) over epochs for all models. Each subplot shows one model with its best validation MAE indicated in the title.}
\label{fig:learning_curves}
\end{figure}

\chapter{Analysis and Discussion}

\section{Understanding the Complexity-Accuracy Tradeoff}

Our results reveal a crucial insight: \textbf{more complex models don't always win}. Here's what we learned:

\subsection{Hybrid Models: The Sweet Spot}

GIN+VN achieves the best accuracy (0.342 MAE) with moderate complexity (1.5M parameters, 351s training time). This suggests that for molecular property prediction, you don't need full global attention—you just need a way to share global information efficiently, which virtual nodes provide.

\subsection{Graph Transformers: Overkill for This Task}

GOAT and Exphormer use 5M+ parameters but achieve lower accuracy than GIN+VN. This suggests that for the ZINC dataset size (~12K graphs), the additional expressiveness of global attention doesn't justify the computational cost. The models may be overfitting or the task may not require the full power of global attention.

\subsection{Simple Baselines: Still Competitive}

GIN achieves 0.374 MAE with only 926K parameters, demonstrating that well-designed local message passing can be highly effective. Sometimes, the simple solution is the best solution.

\section{When Do Graph Transformers Help?}

Based on our results, graph transformers may be more beneficial for:

\begin{itemize}
    \item \textbf{Larger graphs}: Where long-range dependencies are critical and local message passing struggles
    \item \textbf{Complex tasks}: Requiring explicit modeling of global graph structure
    \item \textbf{Diverse datasets}: With more complex graph patterns that benefit from global attention
\end{itemize}

For molecular property prediction on ZINC, local message passing with virtual nodes (hybrid models) appears sufficient. However, we expect graph transformers to show their strengths on larger, more complex graphs.

\section{Limitations and Future Work}

Our study has several limitations that future work should address:

\begin{itemize}
    \item \textbf{Single dataset}: We only evaluated on ZINC; results may differ on other benchmarks like OGB-MolHIV or Peptides
    \item \textbf{Homophily analysis}: We didn't analyze performance on homophilic vs. heterophilic graphs, which could reveal when transformers help most
    \item \textbf{Limited ablations}: We didn't systematically study the impact of positional encodings or virtual node designs
    \item \textbf{Single seed}: Results should be averaged over multiple random seeds for statistical significance
\end{itemize}

Future work should:
\begin{itemize}
    \item Evaluate on larger and more diverse graph datasets
    \item Conduct systematic ablation studies on architectural components
    \item Analyze performance on different graph types (homophilic vs. heterophilic)
    \item Investigate the role of positional encodings in transformer performance
    \item Study scalability on much larger graphs (100K+ nodes)
\end{itemize}

\chapter{Conclusion}

We presented a comprehensive benchmark of scalable graph transformers (GOAT, Exphormer) against GNN baselines and hybrid models on the ZINC molecular property prediction dataset. Our key findings:

\begin{enumerate}
    \item \textbf{Hybrid models win}: GIN+VN achieves the best performance (0.342 MAE) with moderate computational cost, demonstrating that combining expressive GNN architectures with virtual nodes provides the best balance
    \item \textbf{Graph transformers are competitive but costly}: They show competitive results but require significantly more parameters and training time, making them less practical for this task
    \item \textbf{Simple can be effective}: Well-designed local message passing (GIN) remains highly effective for molecular property prediction
    \item \textbf{Complexity doesn't always pay off}: The complexity-accuracy tradeoffs favor hybrid architectures for this dataset size
\end{enumerate}

These results suggest that for molecular property prediction, combining expressive GNN architectures with virtual nodes provides the best balance of accuracy and efficiency. Graph transformers may be more beneficial for larger graphs or tasks requiring explicit global structure modeling, but for the ZINC dataset, the hybrid approach is the clear winner.

The takeaway for practitioners: don't assume that more complex models are always better. Sometimes, the right combination of simple components (like GIN + virtual nodes) outperforms sophisticated architectures while being more efficient and easier to train.

\chapter*{Acknowledgments}
\addcontentsline{toc}{chapter}{Acknowledgments}

We thank the authors of GOAT and Exphormer for making their code publicly available, which made this comprehensive benchmark possible.

\bibliographystyle{unsrt}
\begin{thebibliography}{99}

\bibitem{kong2023goat}
D. Kong, H. Liu, P. Konda, et al. GOAT: A Global Transformer on Large-scale Graphs. ICML 2023.

\bibitem{shirzad2023exphormer}
H. Shirzad, A. Velingker, B. Venkatachalam, D. Sutherland, A. K. Sinop. Exphormer: Sparse Transformers for Graphs. ICML 2023.

\bibitem{kipf2017gcn}
T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. ICLR 2017.

\bibitem{velickovic2018gat}
P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio. Graph attention networks. ICLR 2018.

\bibitem{xu2019gin}
K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? ICLR 2019.

\bibitem{gilmer2017neural}
J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. ICML 2017.

\bibitem{dwivedi2020benchmarking}
V. P. Dwivedi, C. K. Joshi, T. Laurent, Y. Bengio, and X. Bresson. Benchmarking graph neural networks. NeurIPS 2020.

\end{thebibliography}

\end{document}
