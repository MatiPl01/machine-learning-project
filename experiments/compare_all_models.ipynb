{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Por√≥wnanie wszystkich modeli: Graph Transformers vs Baselines vs Hybrid\n",
        "\n",
        "Ten notebook por√≥wnuje wszystkie zaimplementowane modele:\n",
        "\n",
        "## Graph Transformers\n",
        "1. **GOAT** - Global attention z virtual nodes (O(N))\n",
        "2. **Exphormer** - Sparse attention z expander graphs (O(Nd))\n",
        "\n",
        "## Baseline GNNs\n",
        "3. **GCN** - Graph Convolutional Network (O(E))\n",
        "4. **GAT** - Graph Attention Network (O(E))\n",
        "5. **GIN** - Graph Isomorphism Network (O(E)) - **NOWY**\n",
        "6. **GraphMLP** - MLP bez struktury grafu\n",
        "\n",
        "## Modele Hybrydowe\n",
        "7. **GCNVirtualNode** - GCN z Virtual Node (O(E+N)) - **NOWY**\n",
        "8. **GINVirtualNode** - GIN z Virtual Node (O(E+N)) - **NOWY**\n",
        "\n",
        "---\n",
        "\n",
        "## Datasety\n",
        "- **ZINC** (regresja, MAE)\n",
        "- **ogbg-molhiv** (klasyfikacja binarna, ROC-AUC)\n",
        "- **ogbg-molpcba** (multi-label, AP) - **NOWY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "PyTorch version: 2.8.0\n"
          ]
        }
      ],
      "source": [
        "# Setup - dodaj ≈õcie≈ºkƒô do projektu\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, os.path.abspath('..'))\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from torch_geometric.loader import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Importuj wszystkie modele\n",
        "from models import (\n",
        "    GOAT, Exphormer,           # Graph Transformers\n",
        "    GCN, GAT, GIN, GraphMLP,   # Baselines\n",
        "    GCNVirtualNode, GINVirtualNode  # Hybrid\n",
        ")\n",
        "\n",
        "# Importuj utilities\n",
        "from src.utils.data import (\n",
        "    load_zinc_dataset, \n",
        "    load_molhiv_dataset, \n",
        "    load_molpcba_dataset\n",
        ")\n",
        "from src.utils.positional_encodings import precompute_positional_encodings\n",
        "from src.utils.complexity import ComplexityTracker, count_parameters\n",
        "from src.utils.metrics import compute_metrics\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Konfiguracja eksperymentu\n",
        "\n",
        "**Wybierz tryb:**\n",
        "- `cpu` - szybki test (ma≈Çy dataset, ma≈Ço epok) - ~10-15 minut\n",
        "- `gpu` - pe≈Çny eksperyment (ca≈Çy dataset, wiƒôcej epok) - ~1-2 godziny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üñ•Ô∏è  TRYB CPU - Szybki test\n",
            "\n",
            "Konfiguracja:\n",
            "  dataset: zinc\n",
            "  use_subset: True\n",
            "  subset_size: 500\n",
            "  batch_size: 32\n",
            "  num_epochs: 10\n",
            "  hidden_dim: 64\n",
            "  num_layers: 3\n",
            "  num_heads: 4\n",
            "  lr: 0.001\n",
            "  dropout: 0.1\n",
            "  pe_dim: 8\n",
            "  device: cpu\n"
          ]
        }
      ],
      "source": [
        "# ========================================================\n",
        "# KONFIGURACJA - ZMIE≈É TUTAJ\n",
        "# ========================================================\n",
        "\n",
        "EXPERIMENT_MODE = \"cpu\"  # \"cpu\" lub \"gpu\"\n",
        "\n",
        "# ========================================================\n",
        "\n",
        "if EXPERIMENT_MODE == \"cpu\":\n",
        "    # Tryb CPU - szybki test\n",
        "    CONFIG = {\n",
        "        'dataset': 'zinc',\n",
        "        'use_subset': True,\n",
        "        'subset_size': 500,\n",
        "        'batch_size': 32,\n",
        "        'num_epochs': 10,\n",
        "        'hidden_dim': 64,\n",
        "        'num_layers': 3,\n",
        "        'num_heads': 4,\n",
        "        'lr': 1e-3,\n",
        "        'dropout': 0.1,\n",
        "        'pe_dim': 8,\n",
        "        'device': 'cpu',\n",
        "    }\n",
        "    print(\"üñ•Ô∏è  TRYB CPU - Szybki test\")\n",
        "else:\n",
        "    # Tryb GPU - pe≈Çny eksperyment\n",
        "    CONFIG = {\n",
        "        'dataset': 'zinc',\n",
        "        'use_subset': False,\n",
        "        'subset_size': None,\n",
        "        'batch_size': 64,\n",
        "        'num_epochs': 100,\n",
        "        'hidden_dim': 256,\n",
        "        'num_layers': 5,\n",
        "        'num_heads': 8,\n",
        "        'lr': 1e-4,\n",
        "        'dropout': 0.1,\n",
        "        'pe_dim': 16,\n",
        "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    }\n",
        "    print(\"üöÄ TRYB GPU - Pe≈Çny eksperyment\")\n",
        "\n",
        "print(f\"\\nKonfiguracja:\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ≈Åadowanie datasetu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "≈Åadowanie datasetu: zinc\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://www.dropbox.com/s/feo9qle74kg48gy/molecules.zip?dl=1\n",
            "Extracting data/molecules.zip\n",
            "Downloading https://raw.githubusercontent.com/graphdeeplearning/benchmarking-gnns/master/data/molecules/train.index\n",
            "Downloading https://raw.githubusercontent.com/graphdeeplearning/benchmarking-gnns/master/data/molecules/val.index\n",
            "Downloading https://raw.githubusercontent.com/graphdeeplearning/benchmarking-gnns/master/data/molecules/test.index\n",
            "Processing...\n",
            "Processing train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 28204.38it/s]\n",
            "Processing val dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 10073.41it/s]\n",
            "Processing test dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 27112.68it/s]\n",
            "Done!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "U≈ºywam podzbioru: 500 graf√≥w\n",
            "Obliczam positional encodings...\n",
            "Setting up laplacian positional encodings (dim=8)...\n",
            "Precomputing encodings...\n",
            "  Processed 100/10000 graphs\n",
            "  Processed 200/10000 graphs\n",
            "  Processed 300/10000 graphs\n",
            "  Processed 400/10000 graphs\n",
            "  Processed 500/10000 graphs\n",
            "  Processed 600/10000 graphs\n",
            "  Processed 700/10000 graphs\n",
            "  Processed 800/10000 graphs\n",
            "  Processed 900/10000 graphs\n",
            "  Processed 1000/10000 graphs\n",
            "  Processed 1100/10000 graphs\n",
            "  Processed 1200/10000 graphs\n",
            "  Processed 1300/10000 graphs\n",
            "  Processed 1400/10000 graphs\n",
            "  Processed 1500/10000 graphs\n",
            "  Processed 1600/10000 graphs\n",
            "  Processed 1700/10000 graphs\n",
            "  Processed 1800/10000 graphs\n",
            "  Processed 1900/10000 graphs\n",
            "  Processed 2000/10000 graphs\n",
            "  Processed 2100/10000 graphs\n",
            "  Processed 2200/10000 graphs\n",
            "  Processed 2300/10000 graphs\n",
            "  Processed 2400/10000 graphs\n",
            "  Processed 2500/10000 graphs\n",
            "  Processed 2600/10000 graphs\n",
            "  Processed 2700/10000 graphs\n",
            "  Processed 2800/10000 graphs\n",
            "  Processed 2900/10000 graphs\n",
            "  Processed 3000/10000 graphs\n",
            "  Processed 3100/10000 graphs\n",
            "  Processed 3200/10000 graphs\n",
            "  Processed 3300/10000 graphs\n",
            "  Processed 3400/10000 graphs\n",
            "  Processed 3500/10000 graphs\n",
            "  Processed 3600/10000 graphs\n",
            "  Processed 3700/10000 graphs\n",
            "  Processed 3800/10000 graphs\n",
            "  Processed 3900/10000 graphs\n",
            "  Processed 4000/10000 graphs\n",
            "  Processed 4100/10000 graphs\n",
            "  Processed 4200/10000 graphs\n",
            "  Processed 4300/10000 graphs\n",
            "  Processed 4400/10000 graphs\n",
            "  Processed 4500/10000 graphs\n",
            "  Processed 4600/10000 graphs\n",
            "  Processed 4700/10000 graphs\n",
            "  Processed 4800/10000 graphs\n",
            "  Processed 4900/10000 graphs\n",
            "  Processed 5000/10000 graphs\n",
            "  Processed 5100/10000 graphs\n",
            "  Processed 5200/10000 graphs\n",
            "  Processed 5300/10000 graphs\n",
            "  Processed 5400/10000 graphs\n",
            "  Processed 5500/10000 graphs\n",
            "  Processed 5600/10000 graphs\n",
            "  Processed 5700/10000 graphs\n",
            "  Processed 5800/10000 graphs\n",
            "  Processed 5900/10000 graphs\n",
            "  Processed 6000/10000 graphs\n",
            "  Processed 6100/10000 graphs\n",
            "  Processed 6200/10000 graphs\n",
            "  Processed 6300/10000 graphs\n",
            "  Processed 6400/10000 graphs\n",
            "  Processed 6500/10000 graphs\n",
            "  Processed 6600/10000 graphs\n",
            "  Processed 6700/10000 graphs\n",
            "  Processed 6800/10000 graphs\n",
            "  Processed 6900/10000 graphs\n",
            "  Processed 7000/10000 graphs\n",
            "  Processed 7100/10000 graphs\n",
            "  Processed 7200/10000 graphs\n",
            "  Processed 7300/10000 graphs\n",
            "  Processed 7400/10000 graphs\n",
            "  Processed 7500/10000 graphs\n",
            "  Processed 7600/10000 graphs\n",
            "  Processed 7700/10000 graphs\n",
            "  Processed 7800/10000 graphs\n",
            "  Processed 7900/10000 graphs\n",
            "  Processed 8000/10000 graphs\n",
            "  Processed 8100/10000 graphs\n",
            "  Processed 8200/10000 graphs\n",
            "  Processed 8300/10000 graphs\n",
            "  Processed 8400/10000 graphs\n",
            "  Processed 8500/10000 graphs\n",
            "  Processed 8600/10000 graphs\n",
            "  Processed 8700/10000 graphs\n",
            "  Processed 8800/10000 graphs\n",
            "  Processed 8900/10000 graphs\n",
            "  Processed 9000/10000 graphs\n",
            "  Processed 9100/10000 graphs\n",
            "  Processed 9200/10000 graphs\n",
            "  Processed 9300/10000 graphs\n",
            "  Processed 9400/10000 graphs\n",
            "  Processed 9500/10000 graphs\n",
            "  Processed 9600/10000 graphs\n",
            "  Processed 9700/10000 graphs\n",
            "  Processed 9800/10000 graphs\n",
            "  Processed 9900/10000 graphs\n",
            "  Processed 10000/10000 graphs\n",
            "Done!\n",
            "Dataset za≈Çadowany:\n",
            "  Total: 10000 graf√≥w\n",
            "  Train: 400, Val: 50, Test: 50\n",
            "  In channels: 1, Out channels: 1\n",
            "  Task type: regression, Metric: mae\n"
          ]
        }
      ],
      "source": [
        "def load_dataset_by_name(name, use_subset=False, subset_size=500, pe_dim=8):\n",
        "    \"\"\"Za≈Çaduj dataset po nazwie.\"\"\"\n",
        "    print(f\"≈Åadowanie datasetu: {name}\")\n",
        "    \n",
        "    if name == 'zinc':\n",
        "        dataset, split_idx = load_zinc_dataset()\n",
        "        task_type = 'regression'\n",
        "        metric = 'mae'\n",
        "        out_channels = 1\n",
        "        in_channels = dataset[0].x.shape[1] if dataset[0].x.dim() > 1 else 1\n",
        "    elif name == 'molhiv':\n",
        "        dataset, split_idx = load_molhiv_dataset()\n",
        "        task_type = 'binary_classification'\n",
        "        metric = 'rocauc'\n",
        "        out_channels = 1\n",
        "        in_channels = dataset[0].x.shape[1]\n",
        "    elif name == 'molpcba':\n",
        "        dataset, split_idx = load_molpcba_dataset()\n",
        "        task_type = 'multi_label'\n",
        "        metric = 'ap'\n",
        "        out_channels = 128  # 128 tasks\n",
        "        in_channels = dataset[0].x.shape[1]\n",
        "    else:\n",
        "        raise ValueError(f\"Nieznany dataset: {name}\")\n",
        "    \n",
        "    # U≈ºyj podzbioru je≈õli potrzeba\n",
        "    if use_subset:\n",
        "        print(f\"U≈ºywam podzbioru: {subset_size} graf√≥w\")\n",
        "        indices = torch.randperm(len(dataset))[:subset_size]\n",
        "        # Stw√≥rz nowe splity\n",
        "        train_size = int(0.8 * subset_size)\n",
        "        val_size = int(0.1 * subset_size)\n",
        "        split_idx = {\n",
        "            'train': indices[:train_size],\n",
        "            'valid': indices[train_size:train_size+val_size],\n",
        "            'test': indices[train_size+val_size:],\n",
        "        }\n",
        "    \n",
        "    # Precompute positional encodings\n",
        "    print(\"Obliczam positional encodings...\")\n",
        "    dataset = precompute_positional_encodings(dataset, pe_type='laplacian', pe_dim=pe_dim)\n",
        "    \n",
        "    print(f\"Dataset za≈Çadowany:\")\n",
        "    print(f\"  Total: {len(dataset)} graf√≥w\")\n",
        "    print(f\"  Train: {len(split_idx['train'])}, Val: {len(split_idx['valid'])}, Test: {len(split_idx['test'])}\")\n",
        "    print(f\"  In channels: {in_channels}, Out channels: {out_channels}\")\n",
        "    print(f\"  Task type: {task_type}, Metric: {metric}\")\n",
        "    \n",
        "    return dataset, split_idx, {\n",
        "        'task_type': task_type,\n",
        "        'metric': metric,\n",
        "        'in_channels': in_channels,\n",
        "        'out_channels': out_channels,\n",
        "    }\n",
        "\n",
        "# Za≈Çaduj dataset\n",
        "dataset, split_idx, dataset_info = load_dataset_by_name(\n",
        "    CONFIG['dataset'],\n",
        "    use_subset=CONFIG['use_subset'],\n",
        "    subset_size=CONFIG['subset_size'],\n",
        "    pe_dim=CONFIG['pe_dim'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batchy: Train=13, Val=2, Test=2\n"
          ]
        }
      ],
      "source": [
        "# Stw√≥rz DataLoadery\n",
        "train_loader = DataLoader(\n",
        "    dataset[split_idx['train']], \n",
        "    batch_size=CONFIG['batch_size'], \n",
        "    shuffle=True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    dataset[split_idx['valid']], \n",
        "    batch_size=CONFIG['batch_size'], \n",
        "    shuffle=False\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    dataset[split_idx['test']], \n",
        "    batch_size=CONFIG['batch_size'], \n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"Batchy: Train={len(train_loader)}, Val={len(val_loader)}, Test={len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definicja modeli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "MODELE I LICZBA PARAMETR√ìW\n",
            "============================================================\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "unsupported format string passed to dict.__format__",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     90\u001b[0m     params \u001b[38;5;241m=\u001b[39m count_parameters(model)\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m15s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m parametr√≥w\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to dict.__format__"
          ]
        }
      ],
      "source": [
        "def create_models(config, dataset_info):\n",
        "    \"\"\"Stw√≥rz wszystkie modele do por√≥wnania.\"\"\"\n",
        "    in_ch = dataset_info['in_channels']\n",
        "    out_ch = dataset_info['out_channels']\n",
        "    hidden = config['hidden_dim']\n",
        "    layers = config['num_layers']\n",
        "    heads = config['num_heads']\n",
        "    dropout = config['dropout']\n",
        "    pe_dim = config['pe_dim']\n",
        "    \n",
        "    models = {\n",
        "        # ===== Baselines =====\n",
        "        'GCN': GCN(\n",
        "            in_channels=in_ch,\n",
        "            hidden_channels=hidden,\n",
        "            out_channels=out_ch,\n",
        "            num_layers=layers,\n",
        "            dropout=dropout,\n",
        "        ),\n",
        "        'GAT': GAT(\n",
        "            in_channels=in_ch,\n",
        "            hidden_channels=hidden,\n",
        "            out_channels=out_ch,\n",
        "            num_layers=layers,\n",
        "            num_heads=heads,\n",
        "            dropout=dropout,\n",
        "        ),\n",
        "        'GIN': GIN(\n",
        "            in_channels=in_ch,\n",
        "            hidden_channels=hidden,\n",
        "            out_channels=out_ch,\n",
        "            num_layers=layers,\n",
        "            dropout=dropout,\n",
        "        ),\n",
        "        'GraphMLP': GraphMLP(\n",
        "            in_channels=in_ch,\n",
        "            hidden_channels=hidden,\n",
        "            out_channels=out_ch,\n",
        "            num_layers=layers,\n",
        "            dropout=dropout,\n",
        "        ),\n",
        "        \n",
        "        # ===== Hybrid Models =====\n",
        "        'GCN+VN': GCNVirtualNode(\n",
        "            in_channels=in_ch,\n",
        "            hidden_channels=hidden,\n",
        "            out_channels=out_ch,\n",
        "            num_layers=layers,\n",
        "            dropout=dropout,\n",
        "        ),\n",
        "        'GIN+VN': GINVirtualNode(\n",
        "            in_channels=in_ch,\n",
        "            hidden_channels=hidden,\n",
        "            out_channels=out_ch,\n",
        "            num_layers=layers,\n",
        "            dropout=dropout,\n",
        "        ),\n",
        "        \n",
        "        # ===== Graph Transformers =====\n",
        "        'GOAT': GOAT(\n",
        "            in_channels=in_ch,\n",
        "            hidden_channels=hidden,\n",
        "            out_channels=out_ch,\n",
        "            num_layers=layers,\n",
        "            num_heads=heads,\n",
        "            pe_dim=pe_dim,\n",
        "            dropout=dropout,\n",
        "        ),\n",
        "        'Exphormer': Exphormer(\n",
        "            in_channels=in_ch,\n",
        "            hidden_channels=hidden,\n",
        "            out_channels=out_ch,\n",
        "            num_layers=layers,\n",
        "            num_heads=heads,\n",
        "            pe_dim=pe_dim,\n",
        "            dropout=dropout,\n",
        "        ),\n",
        "    }\n",
        "    \n",
        "    return models\n",
        "\n",
        "# Stw√≥rz modele\n",
        "models = create_models(CONFIG, dataset_info)\n",
        "\n",
        "# Poka≈º liczbƒô parametr√≥w\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODELE I LICZBA PARAMETR√ìW\")\n",
        "print(\"=\"*60)\n",
        "for name, model in models.items():\n",
        "    params = count_parameters(model)\n",
        "    # count_parameters zwraca dict z kluczami: 'total', 'trainable', etc.\n",
        "    print(f\"{name:15s}: {params['total']:>10,} parametr√≥w\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Funkcje treningowe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, optimizer, device, task_type='regression'):\n",
        "    \"\"\"Trenuj przez jednƒÖ epokƒô.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        out = model(batch)\n",
        "        \n",
        "        # Oblicz loss w zale≈ºno≈õci od typu zadania\n",
        "        if task_type == 'regression':\n",
        "            y = batch.y.float().view(-1, 1)\n",
        "            loss = F.mse_loss(out, y)\n",
        "        elif task_type == 'binary_classification':\n",
        "            y = batch.y.float().view(-1, 1)\n",
        "            loss = F.binary_cross_entropy_with_logits(out, y)\n",
        "        elif task_type == 'multi_label':\n",
        "            y = batch.y.float()\n",
        "            # Ignoruj NaN labels\n",
        "            mask = ~torch.isnan(y)\n",
        "            loss = F.binary_cross_entropy_with_logits(out[mask], y[mask])\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown task type: {task_type}\")\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item() * batch.num_graphs\n",
        "    \n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, task_type='regression', metric='mae'):\n",
        "    \"\"\"Ewaluuj model.\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        out = model(batch)\n",
        "        \n",
        "        if task_type == 'regression':\n",
        "            y = batch.y.float().view(-1, 1)\n",
        "            loss = F.mse_loss(out, y)\n",
        "            all_preds.append(out.cpu())\n",
        "            all_labels.append(y.cpu())\n",
        "        elif task_type == 'binary_classification':\n",
        "            y = batch.y.float().view(-1, 1)\n",
        "            loss = F.binary_cross_entropy_with_logits(out, y)\n",
        "            all_preds.append(torch.sigmoid(out).cpu())\n",
        "            all_labels.append(y.cpu())\n",
        "        elif task_type == 'multi_label':\n",
        "            y = batch.y.float()\n",
        "            mask = ~torch.isnan(y)\n",
        "            loss = F.binary_cross_entropy_with_logits(out[mask], y[mask])\n",
        "            all_preds.append(torch.sigmoid(out).cpu())\n",
        "            all_labels.append(y.cpu())\n",
        "        \n",
        "        total_loss += loss.item() * batch.num_graphs\n",
        "    \n",
        "    all_preds = torch.cat(all_preds, dim=0)\n",
        "    all_labels = torch.cat(all_labels, dim=0)\n",
        "    \n",
        "    # Oblicz metrykƒô\n",
        "    if metric == 'mae':\n",
        "        score = F.l1_loss(all_preds, all_labels).item()\n",
        "    elif metric == 'rocauc':\n",
        "        from sklearn.metrics import roc_auc_score\n",
        "        try:\n",
        "            score = roc_auc_score(all_labels.numpy(), all_preds.numpy())\n",
        "        except:\n",
        "            score = 0.5\n",
        "    elif metric == 'ap':\n",
        "        from sklearn.metrics import average_precision_score\n",
        "        mask = ~torch.isnan(all_labels)\n",
        "        try:\n",
        "            score = average_precision_score(all_labels[mask].numpy(), all_preds[mask].numpy())\n",
        "        except:\n",
        "            score = 0.0\n",
        "    else:\n",
        "        score = total_loss / len(loader.dataset)\n",
        "    \n",
        "    return score, total_loss / len(loader.dataset)\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, config, dataset_info, verbose=True):\n",
        "    \"\"\"Pe≈Çny trening modelu.\"\"\"\n",
        "    device = torch.device(config['device'])\n",
        "    model = model.to(device)\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=config['num_epochs']\n",
        "    )\n",
        "    \n",
        "    task_type = dataset_info['task_type']\n",
        "    metric = dataset_info['metric']\n",
        "    \n",
        "    # Czy wy≈ºszy wynik jest lepszy?\n",
        "    higher_is_better = metric in ['rocauc', 'ap', 'accuracy']\n",
        "    \n",
        "    best_val_score = float('-inf') if higher_is_better else float('inf')\n",
        "    history = {'train_loss': [], 'val_score': []}\n",
        "    \n",
        "    # Tracking czasu\n",
        "    start_time = time.time()\n",
        "    \n",
        "    pbar = tqdm(range(config['num_epochs']), disable=not verbose)\n",
        "    for epoch in pbar:\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, device, task_type)\n",
        "        val_score, val_loss = evaluate(model, val_loader, device, task_type, metric)\n",
        "        \n",
        "        scheduler.step()\n",
        "        \n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_score'].append(val_score)\n",
        "        \n",
        "        # Update best\n",
        "        if higher_is_better:\n",
        "            if val_score > best_val_score:\n",
        "                best_val_score = val_score\n",
        "        else:\n",
        "            if val_score < best_val_score:\n",
        "                best_val_score = val_score\n",
        "        \n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{train_loss:.4f}',\n",
        "            f'val_{metric}': f'{val_score:.4f}',\n",
        "            f'best': f'{best_val_score:.4f}'\n",
        "        })\n",
        "    \n",
        "    train_time = time.time() - start_time\n",
        "    \n",
        "    return {\n",
        "        'best_val_score': best_val_score,\n",
        "        'history': history,\n",
        "        'train_time': train_time,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trening wszystkich modeli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trenuj wszystkie modele\n",
        "results = {}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TRENING WSZYSTKICH MODELI\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Trenujƒô: {name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Reset modelu (≈õwie≈ºe wagi)\n",
        "    for layer in model.modules():\n",
        "        if hasattr(layer, 'reset_parameters'):\n",
        "            layer.reset_parameters()\n",
        "    \n",
        "    # Mierz pamiƒôƒá\n",
        "    if torch.cuda.is_available() and CONFIG['device'] == 'cuda':\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "    \n",
        "    # Trenuj\n",
        "    result = train_model(\n",
        "        model, \n",
        "        train_loader, \n",
        "        val_loader, \n",
        "        CONFIG, \n",
        "        dataset_info,\n",
        "        verbose=True\n",
        "    )\n",
        "    \n",
        "    # Zapisz wyniki\n",
        "    result['params'] = count_parameters(model)['total']\n",
        "    if torch.cuda.is_available() and CONFIG['device'] == 'cuda':\n",
        "        result['peak_memory_mb'] = torch.cuda.max_memory_allocated() / 1e6\n",
        "    else:\n",
        "        result['peak_memory_mb'] = 0\n",
        "    \n",
        "    results[name] = result\n",
        "    \n",
        "    print(f\"\\n‚úì {name}: Best val {dataset_info['metric']} = {result['best_val_score']:.4f}\")\n",
        "    print(f\"  Czas: {result['train_time']:.1f}s, Parametry: {result['params']:,}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRENING ZAKO≈ÉCZONY\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Podsumowanie wynik√≥w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tabela wynik√≥w\n",
        "import pandas as pd\n",
        "\n",
        "metric = dataset_info['metric']\n",
        "higher_is_better = metric in ['rocauc', 'ap', 'accuracy']\n",
        "\n",
        "summary = []\n",
        "for name, res in results.items():\n",
        "    summary.append({\n",
        "        'Model': name,\n",
        "        f'Val {metric.upper()}': res['best_val_score'],\n",
        "        'Parametry': res['params'],\n",
        "        'Czas (s)': res['train_time'],\n",
        "        'Pamiƒôƒá (MB)': res['peak_memory_mb'],\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(summary)\n",
        "\n",
        "# Sortuj wed≈Çug metryki\n",
        "df = df.sort_values(f'Val {metric.upper()}', ascending=not higher_is_better)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"PODSUMOWANIE WYNIK√ìW - Dataset: {CONFIG['dataset'].upper()}\")\n",
        "print(\"=\"*80)\n",
        "print(df.to_string(index=False))\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wykresy\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# 1. Por√≥wnanie metryk\n",
        "ax = axes[0]\n",
        "names = list(results.keys())\n",
        "scores = [results[n]['best_val_score'] for n in names]\n",
        "colors = ['#2ecc71' if 'VN' in n else '#3498db' if n in ['GOAT', 'Exphormer'] else '#95a5a6' for n in names]\n",
        "bars = ax.barh(names, scores, color=colors)\n",
        "ax.set_xlabel(f'Val {metric.upper()}')\n",
        "ax.set_title(f'Por√≥wnanie modeli ({CONFIG[\"dataset\"]})')\n",
        "for bar, score in zip(bars, scores):\n",
        "    ax.text(bar.get_width(), bar.get_y() + bar.get_height()/2, \n",
        "            f' {score:.4f}', va='center', fontsize=9)\n",
        "\n",
        "# 2. Czas treningu\n",
        "ax = axes[1]\n",
        "times = [results[n]['train_time'] for n in names]\n",
        "ax.barh(names, times, color=colors)\n",
        "ax.set_xlabel('Czas treningu (s)')\n",
        "ax.set_title('Czas treningu')\n",
        "\n",
        "# 3. Parametry vs Wynik\n",
        "ax = axes[2]\n",
        "params = [results[n]['params'] for n in names]\n",
        "for i, name in enumerate(names):\n",
        "    ax.scatter(params[i], scores[i], s=100, c=colors[i], label=name)\n",
        "ax.set_xlabel('Liczba parametr√≥w')\n",
        "ax.set_ylabel(f'Val {metric.upper()}')\n",
        "ax.set_title('Parametry vs Wynik')\n",
        "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nWykres zapisany: model_comparison.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Krzywe uczenia\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, (name, res) in enumerate(results.items()):\n",
        "    if idx >= len(axes):\n",
        "        break\n",
        "    ax = axes[idx]\n",
        "    epochs = range(1, len(res['history']['train_loss']) + 1)\n",
        "    \n",
        "    ax.plot(epochs, res['history']['train_loss'], label='Train Loss', alpha=0.7)\n",
        "    ax2 = ax.twinx()\n",
        "    ax2.plot(epochs, res['history']['val_score'], 'r-', label=f'Val {metric}', alpha=0.7)\n",
        "    \n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss', color='b')\n",
        "    ax2.set_ylabel(f'{metric.upper()}', color='r')\n",
        "    ax.set_title(f'{name}\\nBest: {res[\"best_val_score\"]:.4f}')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Ukryj puste subploty\n",
        "for idx in range(len(results), len(axes)):\n",
        "    axes[idx].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('learning_curves.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nWykres zapisany: learning_curves.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wnioski\n",
        "\n",
        "Po uruchomieniu eksperymentu, wype≈Çnij wnioski:\n",
        "\n",
        "1. **Najlepszy model:** `[TUTAJ]`\n",
        "2. **Czy modele hybrydowe (VN) poprawiajƒÖ wyniki?** `[TAK/NIE]`\n",
        "3. **Czy Graph Transformers sƒÖ lepsze od GNN?** `[TAK/NIE/ZALE≈ªY]`\n",
        "4. **Trade-off czas/jako≈õƒá:** `[OPIS]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zapisz wyniki do pliku\n",
        "import json\n",
        "\n",
        "# Przygotuj wyniki do zapisu\n",
        "save_results = {}\n",
        "for name, res in results.items():\n",
        "    save_results[name] = {\n",
        "        'best_val_score': float(res['best_val_score']),\n",
        "        'params': res['params'],\n",
        "        'train_time': res['train_time'],\n",
        "        'peak_memory_mb': res['peak_memory_mb'],\n",
        "    }\n",
        "\n",
        "output = {\n",
        "    'config': CONFIG,\n",
        "    'dataset_info': dataset_info,\n",
        "    'results': save_results,\n",
        "}\n",
        "\n",
        "filename = f\"results_{CONFIG['dataset']}_{EXPERIMENT_MODE}.json\"\n",
        "with open(filename, 'w') as f:\n",
        "    json.dump(output, f, indent=2)\n",
        "\n",
        "print(f\"Wyniki zapisane do: {filename}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
